<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Using thread concurrency</title>
        <link href="css/style.css" rel="stylesheet" type="text/css"/>
        <meta charset="utf-8"/>
<meta content="urn:uuid:dd0ab1fe-cf2d-4e17-8acb-531b7ebebf1a" name="Adept.expected.resource"/>
    </head>

    <body>
        <section>

                            <header>
                    <h1 class="header-title">Using thread concurrency</h1>
                </header>
            
            <article>
                
<p>Rust threads have the following features:</p>
<ul>
<li>Share memory</li>
<li>Share resources, such as files or sockets</li>
<li>Tend to be thread-safe</li>
<li>Support inter-thread messaging</li>
<li>Are platform-independent</li>
</ul>
<p>For the preceding reasons, we suggest that Rust threads are better suited to most concurrency use cases than subprocesses. If you want to distribute computation, circumvent a blocking operation, or otherwise utilize concurrency for your application—use threads.</p>
<p>To show the thread pattern, we can re-implement the preceding examples. Here are three children threads:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">use std::{thread,time};<br/></span><span class="s1">use std::process;<br/></span><span class="s1">extern crate thread_id;<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   for _ in 0..3 {<br/></span><span class="s1">      thread::spawn(|| {<br/></span><span class="s1">         let t = time::Duration::from_millis(1000);<br/></span><span class="s1">         loop {<br/></span><span class="s1">            println!("child thread #{}:{}", process::id(), <br/>       thread_id::get());<br/></span><span class="s1">            thread::sleep(t);<br/></span><span class="s1">         }<br/></span><span class="s1">      });<br/></span><span class="s1">   }<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      println!("parent thread #{}:{}", process::id(), <br/>      thread_id::get());<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Here, we spawn three threads and let them run. We print the process ID, but we must also print the thread ID because threads share the same process ID. Here is the output demonstrating this:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">parent thread #59804:140735902303104<br/></span><span class="s1">child thread #59804:123145412530176<br/></span><span class="s1">child thread #59804:123145410420736<br/></span><span class="s1">child thread #59804:123145408311296<br/></span><span class="s1">parent thread #59804:140735902303104<br/></span><span class="s1">child thread #59804:123145410420736<br/></span><span class="s1">child thread #59804:123145408311296</span></pre>
<p>The next example to port is the 500 processes and shared memory. In a threaded program, sharing might look something like the following code snippet:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">use std::{thread,time};<br/></span><span class="s1">use std::sync::{Mutex, Arc};<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let mut big_data: Vec&lt;u8&gt; = Vec::with_capacity(200000000);<br/></span><span class="s1">   big_data.push(1);<br/></span><span class="s1">   big_data.push(2);<br/></span><span class="s1">   big_data.push(3);<br/></span><span class="s1">   let big_data = Arc::new(Mutex::new(big_data));<br/></span><span class="s1">   for _ in 0..512 {<br/></span><span class="s1">      let big_data = Arc::clone(&amp;big_data);<br/></span><span class="s1">      thread::spawn(move || {<br/></span><span class="s1">         let t = time::Duration::from_millis(1000);<br/></span><span class="s1">         loop {<br/></span><span class="s1">            let d = big_data.lock().unwrap();<br/></span><span class="s1">            (*d)[2];<br/></span><span class="s1">            thread::sleep(t);<br/></span><span class="s1">         }<br/></span><span class="s1">      });<br/></span><span class="s1">   }<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>The process starts 500 threads, all sharing the same memory. Also, thanks to the lock, we could modify this memory safely if we wanted.</p>
<p>Let's try the server example, as shown in the following code:</p>
<pre class="p1" style="padding-left: 30px"><span class="s1">use std::{thread,time};<br/></span><span class="s1">use std::process;<br/></span><span class="s1">extern crate thread_id;<br/></span><span class="s1">use std::io::prelude::*;<br/></span><span class="s1">use std::net::{TcpListener,TcpStream};<br/></span><span class="s1">use std::sync::{Arc,Mutex};<br/><br/></span><span class="s1">fn serve(incoming: Arc&lt;Mutex&lt;Vec&lt;TcpStream&gt;&gt;&gt;) {<br/></span><span class="s1">   let t = time::Duration::from_millis(10);<br/></span><span class="s1">   loop {<br/>      {<br/></span><span class="s1">         let mut incoming = incoming.lock().unwrap();<br/></span><span class="s1">         for stream in incoming.iter() {<br/></span><span class="s1">            let mut buffer = [0; 2048];<br/></span><span class="s1">            let mut tcp = stream;<br/></span><span class="s1">            tcp.read(&amp;mut buffer).expect("tcp read failed");<br/></span><span class="s1">            let response = format!("respond from #{}:{}\n", <br/>              process::id(), thread_id::get());<br/></span><span class="s1">            tcp.write(response.as_bytes()).expect("tcp write failed");<br/></span><span class="s1">         }<br/></span><span class="s1">         incoming.clear();<br/>      }<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let listener = TcpListener::bind("127.0.0.1:8888").unwrap();<br/></span><span class="s1">   let incoming = Vec::new();<br/></span><span class="s1">   let incoming = Arc::new(Mutex::new(incoming));<br/></span><span class="s1">   for _ in 0..3 {<br/></span><span class="s1">      let incoming = Arc::clone(&amp;incoming);<br/></span><span class="s1">      thread::spawn(move || {<br/></span><span class="s1">         serve(incoming);<br/></span><span class="s1">      });<br/></span><span class="s1">   }<br/></span><span class="s1"><br/></span><span class="s1">   for stream in listener.incoming() {<br/></span><span class="s1">      let mut incoming = incoming.lock().unwrap();<br/></span><span class="s1">      (*incoming).push(stream.unwrap());<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Here, three worker processes scrape a queue of requests that get served down from the parent process. All three children and the parent need to read and mutate the request queue. To mutate the request queue, each thread must lock the data. There is a dance here that the children and parent do to avoid holding the lock for too long. If one thread monopolizes the locked resource, then all other processes wanting to use the data must wait.</p>
<p>The trade-off of locking and waiting is called <strong>contention</strong>. In the worst case scenario, two threads can each hold a lock while waiting for the other thread to release the lock it holds. This is called <strong>deadlock</strong>.</p>
<p>Contention is a difficult problem associated with mutable shared state. For the preceding server case, it would have been better to send messages to children threads. Message passing does not create locks.</p>
<p>Here is a lock-free server:</p>
<pre class="p1">use std::{thread,time};<br/>use std::process;<span class="s1"><br/></span><span class="s1">use std::io::prelude::*;<br/></span><span class="s1">extern crate thread_id;<br/></span><span class="s1">use std::net::{TcpListener,TcpStream};<br/></span><span class="s1">use std::sync::mpsc::{channel,Receiver};<br/></span><span class="s1">use std::collections::VecDeque;<br/><br/></span><span class="s1">fn serve(receiver: Receiver&lt;TcpStream&gt;) {<br/></span><span class="s1">   let t = time::Duration::from_millis(10);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      let mut tcp = receiver.recv().unwrap();<br/></span><span class="s1">      let mut buffer = [0; 2048];<br/></span><span class="s1">      tcp.read(&amp;mut buffer).expect("tcp read failed");<br/></span><span class="s1">      let response = format!("respond from #{}:{}\n", process::id(), <br/>             thread_id::get());<br/></span><span class="s1">      tcp.write(response.as_bytes()).expect("tcp write failed");<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span>}<br/><br/><span class="s1">fn main() {<br/></span><span class="s1">   let listener = TcpListener::bind("127.0.0.1:8888").unwrap();<br/></span><span class="s1">   let mut channels = VecDeque::new();<br/></span><span class="s1">   for _ in 0..3 {<br/></span><span class="s1">      let (sender, receiver) = channel();<br/></span><span class="s1">      channels.push_back(sender);<br/></span><span class="s1">      thread::spawn(move || {<br/></span><span class="s1">         serve(receiver);<br/></span><span class="s1">      });<br/></span><span class="s1">   }<br/></span><span class="s1">   for stream in listener.incoming() {<br/></span><span class="s1">      let round_robin = channels.pop_front().unwrap();<br/></span><span class="s1">      round_robin.send(stream.unwrap()).unwrap();<br/></span><span class="s1">      channels.push_back(round_robin);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Channels work much better in this situation. This multi-threaded server has load balancing controlled from the parent process and does not suffer from lock contention.</p>
<p>Channels are not strictly better than shared state. For example, legitimately contentious resources are good to handle with locks. Consider the following code snippet:</p>
<pre class="p1" style="padding-left: 30px">use std::{thread,time};<span class="s1"><br/></span><span class="s1">extern crate rand;<br/></span><span class="s1">use std::sync::{Arc,Mutex};<br/></span><span class="s1">#[macro_use] extern crate lazy_static;<br/></span><span class="s1">lazy_static! {<br/></span><span class="s1">   static ref NEURAL_NET_WEIGHTS: Vec&lt;Arc&lt;Mutex&lt;Vec&lt;f64&gt;&gt;&gt;&gt; = {<br/></span><span class="s1">      let mut nn = Vec::with_capacity(10000);<br/></span><span class="s1">      for _ in 0..10000 {<br/></span><span class="s1">         let mut mm = Vec::with_capacity(100);<br/></span><span class="s1">         for _ in 0..100 {<br/></span><span class="s1">            mm.push(rand::random::&lt;f64&gt;());<br/></span><span class="s1">         }<br/></span><span class="s1">         let mm = Arc::new(Mutex::new(mm));<br/></span><span class="s1">         nn.push(mm);<br/></span><span class="s1">      }<br/></span><span class="s1">      nn<br/></span><span class="s1">   };<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn train() {<br/></span><span class="s1">   let t = time::Duration::from_millis(100);<br/></span><span class="s1">   loop {<br/></span><span class="s1">      for _ in 0..100 {<br/></span><span class="s1">         let update_position = rand::random::&lt;u64&gt;() % 1000000;<br/></span><span class="s1">         let update_column = update_position / 10000;<br/></span><span class="s1">         let update_row = update_position % 100;<br/></span><span class="s1">         let update_value = rand::random::&lt;f64&gt;();<br/></span><span class="s1">         let mut update_column = NEURAL_NET_WEIGHTS[update_column as usize].lock().unwrap();<br/></span><span class="s1">         update_column[update_row as usize] = update_value;<br/></span><span class="s1">      }<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">}<br/><br/></span><span class="s1">fn main() {<br/></span><span class="s1">   let t = time::Duration::from_millis(1000);<br/></span><span class="s1">   for _ in 0..500 {<br/></span><span class="s1">      thread::spawn(train);<br/></span><span class="s1">   }<br/></span><span class="s1">   loop {<br/></span><span class="s1">      thread::sleep(t);<br/></span><span class="s1">   }<br/></span><span class="s1">}</span></pre>
<p>Here, we have a large mutable data structure (a neural network) that is broken into rows and columns. Each column has a thread-safe lock. Row data is all associated with the same lock. This pattern is useful for data and computation-heavy programs. Neural network training is a good example of where this technique may be relevant. Unfortunately, the code does not implement an actual neural network, but it does demonstrate how lock concurrency could be used to do so.</p>


            </article>

            
        </section>
    </body>

</html>